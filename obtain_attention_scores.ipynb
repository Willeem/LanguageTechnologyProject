{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5998f95cb90545e3b0a3c6ec4f99aa44":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3dbfa63aa68a444eb6294e0c5ab00d27","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f908810a99ae4d06855b6a40920bfca2","IPY_MODEL_efa21547016d47dca31661c0086d76bd"]}},"3dbfa63aa68a444eb6294e0c5ab00d27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f908810a99ae4d06855b6a40920bfca2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0db95959f9e34fd6af58f0625a3111f5","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":4000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f73723ef5e7407299e4142babb64856"}},"efa21547016d47dca31661c0086d76bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8448c0dba2b2481597a1f4378e5a11dd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4000/4000 [00:02&lt;00:00, 1869.80it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_33125890d0924948a13e72b328a98dff"}},"0db95959f9e34fd6af58f0625a3111f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4f73723ef5e7407299e4142babb64856":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8448c0dba2b2481597a1f4378e5a11dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"33125890d0924948a13e72b328a98dff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8393dd872f84d298efa4c8b53441ca3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2dce14b2fd274ccca3e2f38afe081ded","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_916b6fc72da04a5c8cc639b2371d46fc","IPY_MODEL_b82390b0a2894af1bd5e44fa1c3e5ee3"]}},"2dce14b2fd274ccca3e2f38afe081ded":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"916b6fc72da04a5c8cc639b2371d46fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_21364ba564c845138070b6398bc9ce38","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":125,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":125,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0cb4e37e0f2e43cda86dbe117e3b7599"}},"b82390b0a2894af1bd5e44fa1c3e5ee3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_112597268d1e496285c4cf92a9d2031e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 125/125 [00:15&lt;00:00,  8.17it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2faa57b4a0443b1bcdfc5b1c1561278"}},"21364ba564c845138070b6398bc9ce38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0cb4e37e0f2e43cda86dbe117e3b7599":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"112597268d1e496285c4cf92a9d2031e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e2faa57b4a0443b1bcdfc5b1c1561278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da749ee3399c400fb98105ab576df82c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_078b11fabc404ea08f6fcff08eecc2e8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b647d2584cab48fe8fe025a3b64e624c","IPY_MODEL_8c41d194e5ba4d7194b79edb3a44fbe2"]}},"078b11fabc404ea08f6fcff08eecc2e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b647d2584cab48fe8fe025a3b64e624c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5d2a1e3b9a124e6e8064a1fdf63f1784","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2485df108854bb49ea8b46aa95abe0b"}},"8c41d194e5ba4d7194b79edb3a44fbe2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_34b21c716b1445b5a3937457e86ee140","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 2.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_04f3e61ba6254ed984d8a4af4dc3a139"}},"5d2a1e3b9a124e6e8064a1fdf63f1784":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d2485df108854bb49ea8b46aa95abe0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"34b21c716b1445b5a3937457e86ee140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"04f3e61ba6254ed984d8a4af4dc3a139":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"y99af203JHrT","colab_type":"code","outputId":"09769cbc-24c2-40c9-f297-a81579180605","executionInfo":{"status":"ok","timestamp":1591984498342,"user_tz":-120,"elapsed":24185,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z3_n-jgTdEox","colab_type":"code","outputId":"2c6910d1-e0c3-4e27-cfc9-2e8b72bcc224","executionInfo":{"status":"ok","timestamp":1591984509778,"user_tz":-120,"elapsed":35594,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":986}},"source":["!pip install simpletransformers\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting simpletransformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/b6/9bc92d255163b08e932df05462039a77a295086f68c4d92a7a206fce0fc0/simpletransformers-0.34.1-py3-none-any.whl (188kB)\n","\u001b[K     |████████████████████████████████| 194kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.41.1)\n","Collecting tensorboardx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n","Collecting tokenizers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 9.0MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.4)\n","Collecting transformers>=2.11.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n","\u001b[K     |████████████████████████████████| 675kB 29.0MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n","Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 38.0MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 40.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (20.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.15.1)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (47.1.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.11.0->simpletransformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.11.0->simpletransformers) (2.4.7)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.2)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=5da63b6dc7ea1c0e0953a045af8cfc8c20f40eb44856cd7969835bcee028a7b2\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f568b75cdd802648c0ade1bdd25b49a44e125bbae9c45c5c113eff8eddf459ba\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","Installing collected packages: tensorboardx, tokenizers, sacremoses, sentencepiece, transformers, seqeval, simpletransformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 simpletransformers-0.34.1 tensorboardx-2.0 tokenizers-0.7.0 transformers-2.11.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Brn0N5GxciEK","colab_type":"code","outputId":"af46e2b2-93c9-4c7b-cd07-be7044152f2f","executionInfo":{"status":"ok","timestamp":1591984510854,"user_tz":-120,"elapsed":36648,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd drive/Shared\\ drives/Language\\ Technology\\ Project/Final\\ Project/Code/LtP/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/Shared drives/Language Technology Project/Final Project/Code/LtP\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OGVP82wwettX","colab_type":"code","outputId":"cf8967ef-054b-4896-ad21-b171913bc76d","executionInfo":{"status":"ok","timestamp":1591984515776,"user_tz":-120,"elapsed":41532,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","torch.cuda.is_available()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"LiiAq80VfDJx","colab_type":"code","outputId":"cde44167-ca6f-4d85-c947-6a78469be07e","executionInfo":{"status":"ok","timestamp":1591984516116,"user_tz":-120,"elapsed":41847,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%%writefile setup.sh\n","\n","# git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Overwriting setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qWw-0sWDfD-i","colab_type":"code","outputId":"9ce6f1ca-81aa-4995-c326-9c3ef874097a","executionInfo":{"status":"ok","timestamp":1591985145367,"user_tz":-120,"elapsed":671078,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!sh setup.sh"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-fhsx6zaq\n","Created temporary directory: /tmp/pip-req-tracker-sh4mjzxb\n","Created requirements tracker '/tmp/pip-req-tracker-sh4mjzxb'\n","Created temporary directory: /tmp/pip-install-27y98t3l\n","Processing ./apex\n","  Created temporary directory: /tmp/pip-req-build-cjg5t_cj\n","  Added file:///content/drive/Shared%20drives/Language%20Technology%20Project/Final%20Project/Code/LtP/apex to build tracker '/tmp/pip-req-tracker-sh4mjzxb'\n","    Running setup.py (path:/tmp/pip-req-build-cjg5t_cj/setup.py) egg_info for package from file:///content/drive/Shared%20drives/Language%20Technology%20Project/Final%20Project/Code/LtP/apex\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.5.0+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-cjg5t_cj/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-cjg5t_cj/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-cjg5t_cj/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-cjg5t_cj/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-cjg5t_cj/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-cjg5t_cj/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-cjg5t_cj/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-cjg5t_cj has version 0.1, which satisfies requirement apex==0.1 from file:///content/drive/Shared%20drives/Language%20Technology%20Project/Final%20Project/Code/LtP/apex\n","  Removed apex==0.1 from file:///content/drive/Shared%20drives/Language%20Technology%20Project/Final%20Project/Code/LtP/apex from build tracker '/tmp/pip-req-tracker-sh4mjzxb'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-xco7epef\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-cjg5t_cj/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-cjg5t_cj/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-xco7epef/install-record.txt --single-version-externally-managed --compile\n","\n","\n","    torch.__version__  = 1.5.0+cu101\n","\n","\n","    /tmp/pip-req-build-cjg5t_cj/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    creating /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n","    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n","    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n","    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-xco7epef/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-cjg5t_cj\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-sh4mjzxb'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DA0uFblr8lpJ","colab_type":"code","colab":{}},"source":["\n","import pandas as pd\n","import argparse\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from simpletransformers.classification import ClassificationModel\n","from transformers import BertModel, BertTokenizer\n","from os import path\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n","import json "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCTNT2CM8pgQ","colab_type":"code","colab":{}},"source":["def get_test_dataframe():\n","  \"\"\"Returns a pandas dataframe with the correct label\"\"\"\n","  test_en = [[line.rstrip(), 0] for line in open('data/test_src_en.txt', encoding='utf-8').readlines()]\n","  test_deepl = [[line.rstrip(), 1] for line in open('data/test_deepL_en.txt', encoding='utf-8').readlines()]\n","  return pd.DataFrame(test_en + test_deepl)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y22B415U82zH","colab_type":"code","outputId":"a7bef956-9fd4-440d-e0fb-c441a8057619","executionInfo":{"status":"ok","timestamp":1591985193421,"user_tz":-120,"elapsed":719082,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}},"colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["5998f95cb90545e3b0a3c6ec4f99aa44","3dbfa63aa68a444eb6294e0c5ab00d27","f908810a99ae4d06855b6a40920bfca2","efa21547016d47dca31661c0086d76bd","0db95959f9e34fd6af58f0625a3111f5","4f73723ef5e7407299e4142babb64856","8448c0dba2b2481597a1f4378e5a11dd","33125890d0924948a13e72b328a98dff","b8393dd872f84d298efa4c8b53441ca3","2dce14b2fd274ccca3e2f38afe081ded","916b6fc72da04a5c8cc639b2371d46fc","b82390b0a2894af1bd5e44fa1c3e5ee3","21364ba564c845138070b6398bc9ce38","0cb4e37e0f2e43cda86dbe117e3b7599","112597268d1e496285c4cf92a9d2031e","e2faa57b4a0443b1bcdfc5b1c1561278"]}},"source":["\"\"\"Obtain predictions\"\"\"\n","test = get_test_dataframe()\n","model_BERT = ClassificationModel('bert', 'bert_outputs_20_epochs/')\n","predictions, raw_outputs = model_BERT.predict(test[0].to_list())"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5998f95cb90545e3b0a3c6ec4f99aa44","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8393dd872f84d298efa4c8b53441ca3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=125.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tOdmbZUTA0hr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["da749ee3399c400fb98105ab576df82c","078b11fabc404ea08f6fcff08eecc2e8","b647d2584cab48fe8fe025a3b64e624c","8c41d194e5ba4d7194b79edb3a44fbe2","5d2a1e3b9a124e6e8064a1fdf63f1784","d2485df108854bb49ea8b46aa95abe0b","34b21c716b1445b5a3937457e86ee140","04f3e61ba6254ed984d8a4af4dc3a139"]},"outputId":"0409a45e-011a-4441-839f-50af90053c03","executionInfo":{"status":"ok","timestamp":1591985738240,"user_tz":-120,"elapsed":1263876,"user":{"displayName":"W.P. Datema","photoUrl":"","userId":"06209507492226301353"}}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","model = BertModel.from_pretrained('bert_outputs_20_epochs/', output_attentions=True)\n","\"\"\"Creates a set of dictionaries with text, labels, predictions and attention\n","scores to use for the visualisation tool.\"\"\"\n","output_set = list()\n","count = 0\n","for index, row in test.iterrows():\n","  inputs = tokenizer.encode_plus(row[0], return_tensors='pt', add_special_tokens=True) #encode words for compatability with model\n","  input_ids = inputs['input_ids']\n","  attention = model(input_ids)[-1] #obtain attention scores\n","  input_id_list = input_ids[0].tolist() # Batch index 0\n","  tokens = tokenizer.convert_ids_to_tokens(input_id_list) #convert ids back to tokens\n","  all_layers = torch.sum(attention[-1], dim=1).squeeze() / 12 #sum attention scores per word and divide by amount of layers\n","  scores_per_word = (torch.sum(all_layers, dim=0) /len(tokens)).tolist() #Obtain flat list of scores from len(tokens) * len(tokens)\n","  output_dict = {'text': [], 'attention': []}\n","  output_dict['label'] = int(row[1])\n","  output_dict['prediction'] = int(predictions[index])\n","  count += 1\n","  output_dict['id'] = str(count) #neat-vision requires a unique id per sentence\n","  output_dict['text'] = tokens\n","  output_dict['attention'] = scores_per_word\n","  output_set.append(output_dict)\n","with open('text.json', 'w') as outfile:\n","  json.dump(output_set, outfile)\n","\n","#put in json output here: https://cbaziotis.github.io/neat-vision/ \n","\n"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da749ee3399c400fb98105ab576df82c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XPA-EzaTWdaw","colab_type":"code","colab":{}},"source":["label_dict = { \"0\": {\"name\": \"Source translation\", \"desc\": \"Source translation\"}, \n","              \"1\": {\"name\":\"DeepL translation\", \"desc\":\"DeepL translation\"}}\n","with open('labels.json', 'w') as outfile:\n","  json.dump(label_dict, outfile) "],"execution_count":0,"outputs":[]}]}